#### Define functions:
```{r}
rm(list=ls(all=TRUE))


# define functions
f.usePackage = function(p) {
    if (!is.element(p, installed.packages()[,1])) {
        install.packages(p, dep = TRUE);
    }
    require(p, character.only = TRUE);
}

f.getTitle = function(data) {
    title.start = regexpr("\\,[A-Z ]{1,20}\\.", data, TRUE)
    title.end = title.start + attr(title.start, "match.length") - 1
    Title = substr(data, title.start+2, title.end-1)
    return (Title)
}

`%ni%` = Negate(`%in%`)

f.cutoff.optimizer = function(pred, y) {
    output.auc = vector()
    grid = seq(0.1, 0.99, by=0.01)
    for (cut.i in 1:length(grid)) {
        yhat = ifelse(pred >= grid[cut.i], 1, 0)
        result = prediction(yhat, y)
        perf = performance(result,"tpr","fpr")
        auc = performance(result,"auc")
        auc = unlist(slot(auc, "y.values"))
        output.auc = rbind(output.auc, auc)
    }   
    output = cbind(grid, output.auc)
    return(output)
}

f.logloss = function(actual, pred) {
    eps = 1e-15
    nr = nrow(pred)
    pred = matrix(sapply(pred, function(x) max(eps,x)), nrow = nr)      
    pred = matrix(sapply(pred, function(x) min(1-eps,x)), nrow = nr)
    ll = sum(actual*log(pred) + (1-actual)*log(1-pred))
    ll = -ll/nr      
    return(ll);
}

f.cutoff.optimizer = function(pred, y) {
    output.auc <- vector()
    grid = seq(0.1, 0.99, by=0.01)
    for (cut.i in 1:length(grid)) {
        yhat = ifelse(pred >= grid[cut.i], 1, 0)
        result = prediction(yhat, y)
        perf = performance(result,"tpr","fpr")
        auc = performance(result,"auc")
        auc = unlist(slot(auc, "y.values"))
        output.auc = rbind(output.auc, auc)
    }   
    output = cbind(grid=grid, auc=output.auc)
    return(output)
}

f.lasso.modeller = function(x.train, y.train, x.test, y.test){
    # Use cv.glmnet() to determine the best lambda for the lasso:
    lasso.cv = cv.glmnet(x=x.train, y=y.train, alpha=1, family='binomial', type.measure='auc', nfolds=10)
    bestlam.lasso = lasso.cv$lambda.min
    
    # Calculate the predicted outcome of the training and test sets:
    fitted = predict(lasso.cv, s=bestlam.lasso, newx=x.train, type='response')
    pred = predict(lasso.cv, s=bestlam.lasso, newx=x.test, type='response')
    
    y.train = as.numeric(as.character(y.train))
    y.test = as.numeric(as.character(y.test))
    
    # Calculate the log-loss of the test
    logloss = f.logloss(actual=y.test, pred=pred)
    
    # Find the optimal probability cutoff with highest accuracy
    output = f.cutoff.optimizer(fitted, y.train) 
    output = data.frame(output)
    OptimalCutoff = output[which.max(output[,2]),c("grid")]
    
    # Use the optimal probability cutoff to calcuate the accuracy of test data
    pred.binary = ifelse(pred>=OptimalCutoff, 1, 0)
    conf.table = confusionMatrix(table(pred=pred.binary, actual=y.test))
    pred.accuracy = as.numeric(conf.table$overall[1])
    #sens = as.numeric(conf.table$byClass[2])
    #spec = as.numeric(conf.table$byClass[1])
    #kappa = as.numeric(conf.table$overall[2])
    
    rm(pred, fitted, lasso.cv, bestlam.lasso, pred.binary, output)
    
    return(c(logloss, OptimalCutoff, pred.accuracy))
}
```


#### Load libraries:
```{r}
f.usePackage("data.table")
f.usePackage("glmnet")
f.usePackage("dplyr")
f.usePackage("pROC")
f.usePackage("ROCR")
f.usePackage("caret")
```

#### Change working directory:
```{r}
setwd("C:/Users/chang_000/Dropbox/Pinsker/CV_Resume/InterviewQuestions/Avant/avant-analytics-interview/")
```

#### Read data sets:
```{r}
data1 = fread("TitanicData.csv")
data2 = fread("TitanicData2.csv")
data1 = data.frame(data1)
data2 = data.frame(data2)

str(data1)
str(data2)
unique(data1$PassengerId)
unique(data2$PassengerId)
```


## Data Manipulation ##
#### Data1 ####
```{r}
data1$Survived = as.factor(data1$Survived)
data1$Sex = as.factor(ifelse(data1$Sex=="male", 0, 1))
data1$Pclass = as.factor(data1$Pclass)
```

#### Data2 ####
```{r}
# data2
data2$Survived = as.factor(data2$Survived)
data2$Sex = as.factor(data2$Sex)
data2$Pclass = as.factor(data2$Pclass)
data2$Fare = as.numeric(gsub(",","",data2$Fare))
```

#### Merge the two data sets:####
```{r}
variables_list = names(data1)[order(names(data1))]
data = rbind(data1[,variables_list], data2[,variables_list])
dim(data)
summary(data)
```

#### Remove the duplicates if there are any in the merged data set:#### 
```{r}
data = data[!duplicated(data), ]
dim(data)
```

#### Extract some additional information from the `Name` variable: #### 
```{r}
# data$Title
data$Title = f.getTitle(data$Name)
table(data$Title)

# reduce the levels of the "Title" variable
data$Title[data$Title %in% c("Mme","Mlle","Lady","Ms","the Countess", "Jonkheer")] = "OtherFemale"
data$Title[data$Title %in% c("Capt", "Don", "Major", "Sir","Col","Dr","Rev")] = "OtherMale"
data$Title = as.factor(data$Title)
```

#### Detect and impute the missing values: ####
```{r}
apply(is.na(data), 2, sum)
```

#### Impute the missing values of "Age" variable: ####
```{r}
# impute the missing values of "Age" variable
for (i in 1:nrow(data)) {
    if (is.na(data$Age[i])) {
        data$Age[i] = median(data$Age[data$Title==data$Title[i] & data$Pclass==data$Pclass[i]], na.rm=TRUE) 
    } 
}
```

#### Bin Age: ####
```{r}
obj = cut(data$Age, c(seq(0, 60, 10),Inf), labels=c(0:6))
data$Age_bin = as.factor(obj)
```

#### Fare: ####
#### There are two outliers (ID 461 ($2655) and ID 486 ($254667)) in the "Fare" variable. Replace the fares of the outliers with the median of the respective groups: ####
```{r}
data[data$Fare > 1000,]

# Adjust 2 outliers
for (i in 1:nrow(data)) {
    if (data$Fare[i] > 1000) {
        data$Fare[i] = median(data$Fare[data$Title==data$Title[i] & data$Pclass==data$Pclass[i] & data$Embarked==data$Embarked[i]], na.rm=TRUE) 
    } 
}
```

#### Log-transformation of Fare, log(Fare), to make the distribution more close to Gaussian: ####
```{r}
data$logFare = ifelse(data$Fare > 0, log(data$Fare), log(0.001))
```

#### Impute the missing values of Embarked: ####
```{r}
table(data$Embarked) # S is the majority
data$Embarked[data$Embarked == ""] = "S" # impute missing value of Embarked
data$Embarked = as.factor(data$Embarked)
```

#### Save the new data set: ####
```{r}
write.csv(data,"mergeddata.csv", row.names=FALSE)
```


## Produce a glmnet model predicting the chance that a Titanic passanger survived: ##
####  Four models are considered: 
####  Model 1: Survived ~ Age + Fare + Embarked + Pclass + Sex + Title
####  Model 2: Survived ~ Age_bin + Fare + Embarked + Pclass + Sex + Title
####  Model 3: Survived ~ Age + logFare + Embarked + Pclass + Sex + Title
####  Model 4: Survived ~ Age_bin + logFare + Embarked + Pclass + Sex + Title

#
#### K-fold CV (K=10) was adopted to calcuate the means of log-loss and accuracy of the test set of inidividual models, as well as the optimal probability cutoff of the classifiers:

```{r, warning=FALSE}
set.seed(1234)

# set up of K-fold CV for validation
K = 10
block = sample(1:K, nrow(data), replace=TRUE)

# Lasso models:
yvariable = c("Survived")
mod1.xvariables = c("Age","Fare","Embarked","Pclass","Sex","Title")
mod2.xvariables = c("Age_bin","Fare","Embarked","Pclass","Sex","Title")
mod3.xvariables = c("Age","logFare","Embarked","Pclass","Sex","Title")
mod4.xvariables = c("Age_bin","logFare","Embarked","Pclass","Sex","Title")

# initiate tables for the outputs
cv.logloss <- cv.OptimalCutoff <- cv.accuracy <- matrix(0, K, 4)


for (i in 1:K) {
    train.data = data[block!=i,] 
    test.data = data[block==i,] 
    
    #=========================================================
    # Model 1: Age + Embarked + Fare + Pclass + Sex + Title
    
    train = train.data[,c(yvariable,mod1.xvariables)]
    test = test.data[,c(yvariable,mod1.xvariables)]
    
    x.train = model.matrix(Survived~., train)[,-1]
    y.train = train$Survived
    
    x.test = model.matrix(Survived~., test)[,-1]
    y.test = test$Survived
    
    temp = f.lasso.modeller(x.train, y.train, x.test, y.test)
    
    cv.logloss[i,1] = temp[1]
    cv.OptimalCutoff[i,1] = temp[2]
    cv.accuracy[i,1] = temp[3]
    
    rm(train, test, x.train, y.train, x.test, y.test, temp)
    
    #=========================================================
    # Model 2: 
    train = train.data[,c(yvariable,mod2.xvariables)]
    test = test.data[,c(yvariable,mod2.xvariables)]
    
    x.train = model.matrix(Survived~., train)[,-1]
    y.train = train$Survived
    
    x.test = model.matrix(Survived~., test)[,-1]
    y.test = test$Survived
    
    temp = f.lasso.modeller(x.train, y.train, x.test, y.test)
    
    cv.logloss[i,2] = temp[1]
    cv.OptimalCutoff[i,2] = temp[2]
    cv.accuracy[i,2] = temp[3]
    
    rm(train, test, x.train, y.train, x.test, y.test, temp)
    
    #=========================================================
    # Model 3: 
    train = train.data[,c(yvariable,mod3.xvariables)]
    test = test.data[,c(yvariable,mod3.xvariables)]
    
    x.train = model.matrix(Survived~., train)[,-1]
    y.train = train$Survived
    
    x.test = model.matrix(Survived~., test)[,-1]
    y.test = test$Survived
    
    temp = f.lasso.modeller(x.train, y.train, x.test, y.test)
    
    cv.logloss[i,3] = temp[1]
    cv.OptimalCutoff[i,3] = temp[2]
    cv.accuracy[i,3] = temp[3]
    
    rm(train, test, x.train, y.train, x.test, y.test, temp)

    #=========================================================
    # Model 4: 
    train = train.data[,c(yvariable,mod4.xvariables)]
    test = test.data[,c(yvariable,mod4.xvariables)]
    
    x.train = model.matrix(Survived~., train)[,-1]
    y.train = train$Survived
    
    x.test = model.matrix(Survived~., test)[,-1]
    y.test = test$Survived
    
    temp = f.lasso.modeller(x.train, y.train, x.test, y.test)
    
    cv.logloss[i,4] = temp[1]
    cv.OptimalCutoff[i,4] = temp[2]
    cv.accuracy[i,4] = temp[3]
    
    rm(train, test, x.train, y.train, x.test, y.test, temp)

}
```

#### Calculate the means of test log-loss of the models: ####
```{r}
cv.logloss = data.frame(cv.logloss)
colnames(cv.logloss) = c("Model 1", "Model 2", "Model 3", "Model 4")
rownames(cv.logloss) = 1:K
cv.logloss

apply(cv.logloss, 2, mean)
```
Model 3 has the smallest log-loss.


#### Calculate the means of test accuracy of the models: ####
```{r}
cv.accuracy = data.frame(cv.accuracy)
colnames(cv.accuracy) = c("Model 1", "Model 2", "Model 3", "Model 4")
rownames(cv.accuracy) = 1:K
cv.accuracy

apply(cv.accuracy, 2, mean)
```
Model 3 has the highest test accuracy for the survivability prediction.


#### Print the optimal cutoffs for the models in the k-th folds: ####
```{r}
cv.OptimalCutoff = data.frame(cv.OptimalCutoff)
colnames(cv.OptimalCutoff) = c("Model 1", "Model 2", "Model 3", "Model 4")
rownames(cv.OptimalCutoff) = 1:K
cv.OptimalCutoff

#apply(cv.OptimalCutoff, 2, mean)
```


#### Because Model 3 has the smallest test log-loss and the highest test accuracy, build Model 3 using the Full data: ####
```{r}
dataset = data[,c(yvariable,mod3.xvariables)]

x = model.matrix(Survived~., dataset)[,-1]
y = dataset$Survived

# Use cv.glmnet() to determine the best lambda for the lasso:
mod.lasso = cv.glmnet(x=x, y=y, alpha=1, family='binomial', type.measure='auc', nfolds=10)
(bestlam.lasso = mod.lasso$lambda.min)

plot(mod.lasso, main="lasso")

# Accuracy, Sensitivity, Specificity, and Kappa of the new model using the Full dataset
pred = predict(mod.lasso, s=bestlam.lasso, newx=x, type='response')
optimal.cutoff = apply(cv.OptimalCutoff, 2, mean)[3]
pred.binary = ifelse(pred>=optimal.cutoff, 1, 0)
conf.table = confusionMatrix(table(pred=pred.binary, actual=y))
```

#### Calculate the accuracy, sensitivity, specificity, and Kappa of Model 3: ####
```{r}
result = cbind(accuracy = as.numeric(conf.table$overall[1]), 
               sensitivity = as.numeric(conf.table$byClass[2]),
               specificity = as.numeric(conf.table$byClass[1]),
               kappa = as.numeric(conf.table$overall[2]))

result = data.frame(result)
rownames(result) = c("Model_3")
result
```


#### Grep the variables which have non-zero coefficients in the Lasso (Model 3): ####
```{r}
c = coef(mod.lasso, s=bestlam.lasso, exact=TRUE)
inds = which(c!=0)
(variables = row.names(c)[inds])

result.coef = data.frame(cbind(var=variables, coef=c@x))
result.coef = cbind(result.coef, exp(c@x)-1)
colnames(result.coef) = c("var","coef","exp(coef)-1")
result.coef
```

(1) Yonger Passengers had higher survival rate than the elders. The model estimates that, for every one year decrease of age, the odd for survival was estimated to be increased by 0.8%.

(2) Fare had positive influence on the odds for survivability. The model estimates that every 1% increase of fare, the survival rate was increased by 5%.

(3) Passengers embarked at "S" had lower survival rate than Passenger embarked at "C" by 33.8%.

(4) Class 1 passengers had higher survival rate than Class 2 and Class 3 by 33.5% and 80.3%, respectively.

(5) Female passengers had higher survival rate than male passengers.

(6) Relative to passengers with title of Master, "Mrs" had higher survival rate by 22.2%.
And "Master" had higher survival rate than "Mr" and other kinds of male titles by 81.8% and 73.6%, respectively.



#### Adding Gender*Pclass terms to Model 3:

#### Model 3a: Survived ~ Age + logFare + Embarked + Pclass + Sex + Title + Male_Pclass1 + Male_Pclass2 + Male_Pclass3 + Female_Pclass1 + Female_Pclass2 + Female_Pclass3
```{r, warning=FALSE}
dataNew = data
dataNew$Male_Pclass1 = as.factor(ifelse((data$Sex==0 & data$Pclass==1),1,0))
dataNew$Male_Pclass2 = as.factor(ifelse((data$Sex==0 & data$Pclass==2),1,0))
dataNew$Male_Pclass3 = as.factor(ifelse((data$Sex==0 & data$Pclass==3),1,0))
dataNew$Female_Pclass1 = as.factor(ifelse((data$Sex==1 & data$Pclass==1),1,0))
dataNew$Female_Pclass2 = as.factor(ifelse((data$Sex==1 & data$Pclass==2),1,0))
dataNew$Female_Pclass3 = as.factor(ifelse((data$Sex==1 & data$Pclass==3),1,0))

mod3a.xvariables = c("Age","logFare","Embarked","Pclass","Sex","Title","Male_Pclass1",
                     "Male_Pclass2","Male_Pclass3","Female_Pclass1","Female_Pclass2",
                     "Female_Pclass3")
yvariable = c("Survived")

dataset = dataNew[,c(yvariable,mod3a.xvariables)]

# set up of K-fold CV for validation
K = 10
block = sample(1:K, nrow(dataset), replace=TRUE)

# initiate tables for the outputs
cv.result <- matrix(0, K, 3)

for (i in 1:K) {
    train.data = dataset[block!=i,] 
    test.data = dataset[block==i,] 
    
    #=========================================================
    # Model 3a: Age+logFare+Embarked+Pclass+Sex+Title+Male_Pclass1+Male_Pclass2+Male_Pclass3
    #           +Female_Pclass1+Female_Pclass2+Female_Pclass3
    
    train = train.data[,c(yvariable,mod3a.xvariables)]
    test = test.data[,c(yvariable,mod3a.xvariables)]
    
    x.train = model.matrix(Survived~., train)[,-1]
    y.train = train$Survived
    
    x.test = model.matrix(Survived~., test)[,-1]
    y.test = test$Survived
    
    temp = f.lasso.modeller(x.train, y.train, x.test, y.test)
    
    cv.result[i,1] = temp[1]
    cv.result[i,2] = temp[2]
    cv.result[i,3] = temp[3]
    
    rm(train.data, test.data, x.train, y.train, x.test, y.test, temp)
}

# output
cv.result = data.frame(cv.result)
colnames(cv.result) = c("log-loss", "optimal.cutoff", "test.accuracy")
rownames(cv.result) = 1:K
cv.result

apply(cv.result, 2, mean)


# Build Model 3a using the Full data
x = model.matrix(Survived~., dataset)[,-1]
y = dataset$Survived

mod.lasso = cv.glmnet(x=x, y=y, alpha=1, family='binomial', type.measure='auc', nfolds=10)
(bestlam.lasso = mod.lasso$lambda.min)

# Predict using Model 3a
pred = predict(mod.lasso, s=bestlam.lasso, newx=x, type='response')

# Calculate the log-loss of Model 3a
f.logloss(as.numeric(as.character(y)), pred) 

# Calcuate the accuracy of Model 3a
optimal.cutoff = apply(cv.result, 2, mean)[2]
pred.binary = ifelse(pred>=optimal.cutoff, 1, 0)
conf.table = confusionMatrix(table(pred=pred.binary, actual=y))

result.3a = cbind(accuracy = as.numeric(conf.table$overall[1]), 
                  sensitivity = as.numeric(conf.table$byClass[2]),
                  specificity = as.numeric(conf.table$byClass[1]),
                  kappa = as.numeric(conf.table$overall[2]))
result.3a = data.frame(result.3a)
rownames(result.3a) = c("Model_3a")
result.3a = rbind(result, result.3a)
result.3a
```

Adding Gender*Pclass terms to the Model slightly improved the predicting performance in accuracy, sensitivity, and Kappa.

####  Model 3: Survived ~ Age + logFare + Embarked + Pclass + Sex + Title

#### Model 3a: Survived ~ Age + logFare + Embarked + Pclass + Sex + Title + Male_Pclass1 + Male_Pclass2 + Male_Pclass3 + Female_Pclass1 + Female_Pclass2 + Female_Pclass3

